{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### What is this code?\n",
    "- TODO: submission for the VAE project by Joel Amarou Heuer\n",
    "- TODO: theoretical fundamentals: Link to paper\n",
    "\n",
    "### Flaws of Paper\n",
    "- TODO: Methodology not precise \n",
    "- TODO: include Stackoverflow comments\n",
    "\n",
    "### Experiments \n",
    "\n",
    "### Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# SOFTWARE DEVELOPMENT\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "# DATA SCIENCE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score, f1_score\n",
    "\n",
    "# DEEP LEARNING\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.optim import Optimizer, Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEArchitecture:\n",
    "    '''\n",
    "    Helper class for defining the architecture of a VAE.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder:nn.Module,\n",
    "        latent_mu:nn.Module,\n",
    "        latent_sigma:nn.Module,\n",
    "        decoder:nn.Module,\n",
    "        recon_mu:nn.Module,\n",
    "        recon_sigma:nn.Module\n",
    "    ):\n",
    "        self.encoder = encoder\n",
    "        self.latent_mu = latent_mu\n",
    "        self.latent_sigma = latent_sigma\n",
    "        self.decoder = decoder\n",
    "        self.recon_mu = recon_mu\n",
    "        self.recon_sigma = recon_sigma\n",
    "        pass\n",
    "\n",
    "class VAELogTrain():\n",
    "    '''\n",
    "    Helper class for logging training progress of VAE\n",
    "    '''\n",
    "    def __init__(\n",
    "            self, \n",
    "            loss:[float],\n",
    "            kl: [float],\n",
    "            recon_loss: [float]      \n",
    "        ) -> None:\n",
    "        self.loss = loss\n",
    "        self.kl = kl \n",
    "        self.recon_loss = recon_loss\n",
    "        pass\n",
    "\n",
    "\n",
    "'''Enum for model selection'''\n",
    "class ModelToTrain(Enum):\n",
    "    CNN_MNIST = 1,\n",
    "    FULLY_TABULAR = 2\n",
    "\n",
    "\n",
    "'''Enum representing all possible classes in MNIST'''\n",
    "class LabelsMNIST(int, Enum):\n",
    "    Zero = 0,\n",
    "    One = 1,\n",
    "    Two = 2,\n",
    "    Three = 3,\n",
    "    Four = 4,\n",
    "    Five = 5,\n",
    "    Six = 6,\n",
    "    Seven = 7,\n",
    "    Eight = 8,\n",
    "    Nine = 9\n",
    "\n",
    "@dataclass\n",
    "class StrIntMapping:\n",
    "    label:str\n",
    "    encoded:int\n",
    "\n",
    "'''Enum representing all possible classes in KDD1999'''\n",
    "class LabelsKDD1999(Enum):\n",
    "    Normal = StrIntMapping('normal', 0)\n",
    "    Probe = StrIntMapping('probe', 1)\n",
    "    DoS = StrIntMapping('dos', 2)\n",
    "    U2R = StrIntMapping('u2r', 3)\n",
    "    R2L = StrIntMapping('r2l', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDataset(ABC):\n",
    "    '''Interface / Abstract class for all datasets to train on'''\n",
    "    @abstractmethod\n",
    "    def get_data(self) -> TensorDataset:\n",
    "        pass     \n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self):\n",
    "        pass \n",
    "\n",
    "    @abstractmethod\n",
    "    def normalize(self):\n",
    "        pass \n",
    "\n",
    "    @abstractmethod\n",
    "    def to_tensor_dataset(self) -> TensorDataset:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_anomaly_train_test(self, \n",
    "                X_train:np.ndarray,\n",
    "                y_train:np.ndarray,\n",
    "                X_test:np.ndarray,\n",
    "                y_test:np.ndarray\n",
    "                ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"splits dataset into train (only normal) and test (normal, and anomaly) set\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class DatasetMNIST(IDataset):\n",
    "    def __init__(self, is_debug=True) -> None:\n",
    "        super().__init__()\n",
    "        self.is_debug = is_debug\n",
    "        pass \n",
    "\n",
    "    def get_data(\n",
    "            self, \n",
    "            anomaly_class:LabelsMNIST = LabelsMNIST.Two\n",
    "        ) -> Tuple[TensorDataset,TensorDataset]:\n",
    "        print(f'LOADING DATA (anomaly class = {anomaly_class.value}):') if self.is_debug else ''\n",
    "\n",
    "        # Load & pre-process data\n",
    "        train_dataset, test_dataset = self.load()\n",
    "        X_train, y_train = self.reshape(train_dataset)\n",
    "        X_test, y_test = self.reshape(test_dataset)\n",
    "\n",
    "        # extract anomaly class from train and add anomaly to test\n",
    "        X_train, y_train, X_test, y_test = self.get_anomaly_train_test(\n",
    "            X_train, y_train, \n",
    "            X_test, y_test, \n",
    "            anomaly_class=anomaly_class)\n",
    "    \n",
    "        # normalize pixel range from [0,255] to [0,1]\n",
    "        X_train, X_test = self.normalize(X_train, X_test)\n",
    "\n",
    "        # convert to TensorDataset\n",
    "        dataset_train:TensorDataset = self.to_tensor_dataset(X_train, y_train) \n",
    "        dataset_test:TensorDataset = self.to_tensor_dataset(X_test, y_test)\n",
    "\n",
    "        print('\\t\\t(✓) loaded data\\n') if self.is_debug else ''\n",
    "        return dataset_train, dataset_test\n",
    "    \n",
    "    def load(self) -> Tuple[torch.utils.data.Dataset, torch.utils.data.Dataset]:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        train_dataset = torchvision.datasets.MNIST(root='../../data', train=True, transform=transform, download=True)\n",
    "        test_dataset = torchvision.datasets.MNIST(root='../../data', train=False, transform=transform, download=True)\n",
    "\n",
    "        print('\\t\\t(✓) downloaded data (train & test)') if self.is_debug else ''\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def reshape(self, dataset) -> Tuple[np.ndarray,np.ndarray]:\n",
    "        y = dataset.targets\n",
    "        X = dataset.data\n",
    "        X = X.view(-1, 1, 28, 28)  # Reshape to [batch_size, channels, height, width]\n",
    "        print('\\t\\t(✓) reshaped X for train/test') if self.is_debug else ''\n",
    "        return X,y\n",
    "\n",
    "    def normalize(self, X_train:np.ndarray, X_test:np.ndarray) -> Tuple[np.ndarray,np.ndarray]:\n",
    "        '''normalizes pixel range from [0,255] to [0,1]'''\n",
    "        X_train_new = X_train / 255\n",
    "        X_test_new = X_test / 255\n",
    "\n",
    "        print(f'\\t\\t(✓) normalized pixel range from [{X_train.min()}, {X_train.max()}] to[{X_train_new.min()}, {X_train_new.max()}]') if self.is_debug else ''\n",
    "        return X_train_new, X_test_new\n",
    "\n",
    "\n",
    "    def get_anomaly_train_test(self, \n",
    "                X_train:np.ndarray,\n",
    "                y_train:np.ndarray,\n",
    "                X_test:np.ndarray,\n",
    "                y_test:np.ndarray,\n",
    "                anomaly_class:LabelsMNIST\n",
    "                ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"splits dataset into train (only normal) and test (normal, and anomaly) set\"\n",
    "\n",
    "        # add anomaly class to test\n",
    "        X_test = torch.cat([X_test, X_train[y_train == anomaly_class.value]], dim=0)\n",
    "        y_test = torch.cat([y_test, y_train[y_train == anomaly_class.value]], dim=0)\n",
    "\n",
    "        # remove anomaly class from train\n",
    "        X_train = X_train[y_train != anomaly_class.value]\n",
    "        y_train = y_train[y_train != anomaly_class.value]\n",
    "\n",
    "        print(f'\\t\\t(✓) Training set only contains NORMALS, NO ANOMALIES CLASS {anomaly_class.value}.') if self.is_debug else ''\n",
    "        print(f'\\t\\t\\tlabels:\\t{y_train.unique().tolist()}') if self.is_debug else ''\n",
    "        print(f'\\t\\t(✓) Test set contains NORAMLS and ANOMALY CLASS {anomaly_class.value}.') if self.is_debug else ''\n",
    "        print(f'\\t\\t\\tlabels:\\t{y_test.unique().tolist()}') if self.is_debug else ''\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def to_tensor_dataset(self, X:np.ndarray,y:np.ndarray ) -> TensorDataset:\n",
    "        tensor_dataset:TensorDataset = TensorDataset(X,y)\n",
    "        print('\\t\\t(✓) casted X,y to TensorDataset for train/test') if self.is_debug else ''\n",
    "        return tensor_dataset\n",
    "\n",
    "        \n",
    "class DatasetKDD(IDataset):\n",
    "    def __init__(self, is_debug=True) -> None:\n",
    "        super().__init__()\n",
    "        self.is_debug = is_debug\n",
    "        pass \n",
    "\n",
    "    def get_data(self, anomaly_class:LabelsKDD1999) -> [TensorDataset, TensorDataset]:\n",
    "        print('LOADING DATA:') if self.is_debug else ''\n",
    "        # Load & pre-process data (dataframe)\n",
    "        df:pd.DataFrame = self.load()\n",
    "        df = self.fix_dtypes(df)\n",
    "        df = self.normalize(df)\n",
    "        df = self.one_hot_encoding(df)\n",
    "\n",
    "        # drop all rows with Attack Type = nan\n",
    "        df = df.dropna(subset=['Attack Type'])\n",
    "\n",
    "        # split into train and test\n",
    "        df_train:pd.DataFrame = df.sample(frac=0.8, random_state=42)\n",
    "        df_test:pd.DataFrame = df.drop(df_train.index)\n",
    "\n",
    "        # extract anomaly class from df_train and add anomaly to df_test\n",
    "        df_train, df_test = self.get_anomaly_train_test(df_train, df_test, anomaly_class)\n",
    "    \n",
    "        # split into X, y(encoded) \n",
    "        X_train, y_train_encoded = self.get_X_Yencoded(df_train)\n",
    "        X_test, y_test_encoded = self.get_X_Yencoded(df_test)\n",
    "\n",
    "        # to TensorDataset (DataLoader expects Dataset)\n",
    "        dataset_train:TensorDataset = self.to_tensor_dataset(X_train, y_train_encoded)\n",
    "        dataset_test:TensorDataset = self.to_tensor_dataset(X_test, y_test_encoded)\n",
    "\n",
    "        print('\\t\\t(✓) loaded data\\n') if self.is_debug else ''\n",
    "        return dataset_train, dataset_test\n",
    "     \n",
    "    def load(self) -> pd.DataFrame:\n",
    "        '''load local data from directory and setup a dataframe'''\n",
    "\n",
    "        # FETCH DATA: 10% labeled data\n",
    "        ### 1. Load\n",
    "        df = pd.read_csv('../../data/kddcup.data_10_percent.gz', header=None)\n",
    "        cols = pd.read_csv('../../data/kddcup.names',header=None)\n",
    "        print('\\t\\t(✓) downloaded labeled data') if self.is_debug else ''\n",
    "        \n",
    "        ### 2. Add column names to DataFrame\n",
    "        if cols[0][0] == 'back':\n",
    "            cols = cols.drop(cols.index[0])\n",
    "            cols.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        cols = cols.dropna(axis=1)\n",
    "         \n",
    "        ### split merged column names (name:type --> name | type)\n",
    "        cols[[0,1]] = cols[0].str.split(':',expand = True)\n",
    "\n",
    "        ### add column names to DataFrame\n",
    "        names = cols[0].tolist()\n",
    "        names.append('label')\n",
    "        df.columns = names\n",
    "\n",
    "        ### 3. Rename y-labels and show their occurances\n",
    "        df['label'] = df['label'].str.replace('.', '', regex=False)\n",
    "        df.groupby(['label']).size().sort_values()\n",
    "\n",
    "        #--------------------------------\n",
    "        # FETCH DATA: ATTACK TYPE  (Summarize labels (i. e., attack types))\n",
    "        ### 1. Download Attack Types \n",
    "        df_attack_types = pd.read_csv('../../data/training_attack_types')\n",
    "        print('\\t\\t(✓) downloaded attack type data') if self.is_debug else ''\n",
    "\n",
    "        ### 2. Split columns (fetched data contains two features in one column, so split it)\n",
    "        df_temp = pd.DataFrame(columns=['Attack','Type'])\n",
    "        df_temp[['Attack','Type']] = df_attack_types['back dos'].str.split(' ', expand=True)\n",
    "\n",
    "        row_normal = pd.DataFrame({'Attack': 'normal', 'Type':'normal'}, index=[0]) # add normal to attacks\n",
    "        df_temp = pd.concat([df_temp, row_normal], ignore_index=True)\n",
    "\n",
    "        ### 3. Add column 'Attack Type' to df \n",
    "        df['Attack Type'] = df['label'].map(df_temp.set_index('Attack')['Type'])\n",
    "\n",
    "        ### 4. Rearrange columns\n",
    "        cols = list(df.columns)\n",
    "        cols.insert(0, cols.pop(cols.index('Attack Type')))\n",
    "        df = df.loc[:, cols]\n",
    "        cols.insert(1, cols.pop(cols.index('label')))\n",
    "        df = df.loc[:, cols]\n",
    "        return df \n",
    "\n",
    "    def fix_dtypes(self, df:pd.DataFrame)-> pd.DataFrame:\n",
    "        ''' Some columns are categorical (0,1) \n",
    "            but due to the import they are considered to be numerical\n",
    "            '''\n",
    "        cols_categorical = ['protocol_type', 'service', 'flag', 'land', 'logged_in', 'is_host_login', 'is_guest_login']\n",
    "        df[cols_categorical] = df[cols_categorical].astype(str)\n",
    "        print(f'''\\t\\t(✓) fixed dtypes (int -> str) \n",
    "        \\twith len =\\t\\t{len(df.select_dtypes(exclude=[float, int]).columns )} \n",
    "        \\twith columns =\\t{df.select_dtypes(exclude=[float, int]).columns.values}''') if self.is_debug else ''\n",
    "        return df\n",
    "\n",
    "    def normalize(self, df:pd.DataFrame)-> pd.DataFrame:\n",
    "        '''Normalize numerical values into [0, 1] range using Min-Max scaling.'''\n",
    "     \n",
    "        # Get numerical columns\n",
    "        df_numerical = df.select_dtypes(include=[float, int])\n",
    "        \n",
    "        # Normalize using Min-Max scaling handling division by zero\n",
    "        min_max_scaler = lambda x: (x - x.min()) / (x.max() - x.min()) if (x.max() - x.min()) != 0 else x\n",
    "        df_normalized = df_numerical.apply(min_max_scaler)\n",
    "        \n",
    "        # Combine the normalized numerical columns with the non-numerical columns\n",
    "        df[df_normalized.columns] = df_normalized\n",
    "\n",
    "        print('\\t\\t(✓) normalized numerical columns into [0, 1] range') if self.is_debug else ''\n",
    "        return df\n",
    "    \n",
    "    def one_hot_encoding(self,df:pd.DataFrame)-> pd.DataFrame:\n",
    "        ### Remove y-variables for one-hot-encoding\n",
    "        df_no_ylabel = df.iloc[:, 2::1]\n",
    "        non_number_cols = df_no_ylabel.select_dtypes(exclude=[float, int]).columns\n",
    "        df_encoded = pd.get_dummies(df_no_ylabel, columns=non_number_cols).astype(float)\n",
    "\n",
    "        ### merge y-variable with one-hot encoded features\n",
    "        df = pd.concat([df.iloc[:, 0:2:1], df_encoded], axis=1)\n",
    "        print('\\t\\t(✓) one-hot encoded categorical columns') if self.is_debug else ''\n",
    "\n",
    "        return df \n",
    "\n",
    "    def get_X_Yencoded(self, df:pd.DataFrame)-> Tuple[np.ndarray,np.ndarray]:\n",
    "        ''' splits data into X and y\n",
    "            and encodes y\n",
    "        '''\n",
    "        # split into X,y \n",
    "        X:pd.DataFrame = df.iloc[:, 2:]\n",
    "        y:pd.DataFrame = df.iloc[:, :1] # only use ['Attack Type'], not ['label']\n",
    "\n",
    "        # X to numpy\n",
    "        X:np.ndarray = X.values \n",
    "\n",
    "        # create encoded y based LabelKDD1999\n",
    "        y_encoded:np.ndarray = y['Attack Type'].map(\n",
    "            {class_label.value.label : class_label.value.encoded for class_label in LabelsKDD1999}).values\n",
    "\n",
    "        print('\\t\\t(✓) casted DataFrame into X, y (y is one-hot encoded)') if self.is_debug else ''\n",
    "        return X,y_encoded\n",
    "        \n",
    "    def get_anomaly_train_test(\n",
    "            self,\n",
    "            df_train:pd.DataFrame,\n",
    "            df_test:pd.DataFrame,\n",
    "            anomaly_class:LabelsKDD1999                   \n",
    "        ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"splits dataset into train (only normal) and test (normal, and anomaly) set\"\n",
    "        \n",
    "        # add anomaly class to test\n",
    "        df_test = pd.concat([df_test, df_train[df_train['Attack Type'] == anomaly_class.value.label]], ignore_index=True)\n",
    "\n",
    "        # remove anomaly class from train\n",
    "        df_train = df_train[df_train['Attack Type'] != anomaly_class.value.label]\n",
    "\n",
    "        print(f'\\t\\t(✓) Training set only contains NORMALS, NO ANOMALIES CLASS {anomaly_class.value.label}.') if self.is_debug else ''\n",
    "        print(f'\\t\\t\\tlabels:\\t{df_train[\"Attack Type\"].unique().tolist()}') if self.is_debug else ''\n",
    "        print(f'\\t\\t(✓) Test set contains NORAMLS and ANOMALY CLASS {anomaly_class.value.label}.') if self.is_debug else ''\n",
    "        print(f'\\t\\t\\tlabels:\\t{df_test[\"Attack Type\"].unique().tolist()}') if self.is_debug else ''\n",
    "        return df_train, df_test\n",
    "\n",
    "    def to_tensor_dataset(self, X:np.ndarray, y:np.ndarray) -> TensorDataset:\n",
    "        '''\n",
    "        transform X,y to `TensorDataset`, since `DataLoader` expects it for training\n",
    "        '''\n",
    "        # to Dataset (DataLoader expects Dataset)\n",
    "        X_tensor:torch.Tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor:torch.Tensor  = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # floatify\n",
    "        X_tensor = X_tensor.float()\n",
    "        y_tensor = y_tensor.float()\n",
    "\n",
    "        dataset:TensorDataset = TensorDataset(X_tensor, y_tensor)\n",
    "        print('\\t\\t(✓) casted X,y to TensorDataset') if self.is_debug else ''\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "Interface/Abstract class dictating the overall behaviour (forward, loss, etc.) of the models. Each implementation of it defines the architecture (layers, activation functions, etc).\n",
    "\n",
    "IMPORTANT \n",
    "\n",
    "`is_probabilistic` is a flag that indicates whether the model is probabilistic or not. This is important for the loss calculation:\n",
    "- True <--> KL & reconstruction probability\n",
    "- False <--> KL & reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IVAE(nn.Module, ABC):\n",
    "    def __init__(self, \n",
    "                 io_size:int,\n",
    "                latent_size:int,\n",
    "                is_probabilistic:bool,\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        self.is_probabilistic = is_probabilistic # determines whether VAE is probabilistic or not\n",
    "        self.io_size = io_size\n",
    "        self.latent_size = latent_size\n",
    "        self.prior =  Normal(0,1)\n",
    "        \n",
    "        # architecture\n",
    "        architecture:VAEArchitecture= self.get_architecture()\n",
    "        self.encoder:nn.Module =  architecture.encoder\n",
    "        self.latent_mu:nn.Module = architecture.latent_mu\n",
    "        self.latent_sigma:nn.Module = architecture.latent_sigma\n",
    "        self.decoder:nn.Module = architecture.decoder\n",
    "        \n",
    "        # probabilistic architecture\n",
    "        self.recon_mu:nn.Module = architecture.recon_mu\n",
    "        self.recon_sigma:nn.Module = architecture.recon_sigma\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_architecture(self)  -> VAEArchitecture:\n",
    "        pass \n",
    "\n",
    "    #=================[FORWARD PASS]==============\n",
    "    def forward(self, x: Tensor) -> dict:\n",
    "        x = x.float()  # Convert input to torch.float32\n",
    "\n",
    "        # PREDICTION\n",
    "        pred_result = self.predict(x)\n",
    "        \n",
    "        # LOSS \n",
    "        loss_dict:dict = self.get_loss(x, pred_result)\n",
    "\n",
    "        return dict(**loss_dict, **pred_result)\n",
    "\n",
    "    def predict(self, x) -> dict:\n",
    "        # INPUT\n",
    "        x = x.float()  # Convert input to torch.float32\n",
    "        batch_size = len(x)\n",
    "        shape:[int] = list(x.shape)\n",
    "\n",
    "        # ENCODING\n",
    "        x_encoded = self.encoder(x)\n",
    "\n",
    "        # LATENT SPACE - softplus to ensure values are positive\n",
    "        latent_mu = self.latent_mu(x_encoded) \n",
    "        latent_sigma = softplus(self.latent_sigma(x_encoded)) \n",
    "\n",
    "        dist = Normal(latent_mu, latent_sigma)\n",
    "\n",
    "        # SAMPLE FROM LATENT SPACE with repameterization trick \n",
    "        z = dist.rsample()\n",
    "        z = z.view(batch_size, self.latent_size)\n",
    "    \n",
    "        # DECODER \n",
    "        decoded = self.decoder(z)\n",
    "\n",
    "        # PROBABILISTIC\n",
    "        recon_mu = None\n",
    "        recon_sigma = None\n",
    "\n",
    "        if self.is_probabilistic:\n",
    "            recon_mu = self.recon_mu(decoded)\n",
    "            recon_sigma = softplus(self.recon_sigma(decoded))\n",
    "\n",
    "        return dict(\n",
    "            latent_sigma=latent_sigma, \n",
    "            latent_dist = dist,\n",
    "            z = z,\n",
    "            decoded=decoded,       \n",
    "            recon_mu=recon_mu, \n",
    "            recon_sigma=recon_sigma\n",
    "        )\n",
    "    \n",
    "    def get_reconstruction_loss(self, x:Tensor, pred_result:dict) -> Tensor:\n",
    "        ''' calculates reconstruction loss for each instance in batch\n",
    "           - pVAE: log  likelihood\n",
    "            - dVAE: mse loss\n",
    "\n",
    "        no KL divergence is calculated\n",
    "\n",
    "\n",
    "        returns \n",
    "        [batch_size] Tensor, \n",
    "        each entry is the reconstruction loss for the corresponding instance in the batch\n",
    "        '''\n",
    "        recon_loss = None\n",
    "\n",
    "        # probablistic VAE\n",
    "        if self.is_probabilistic:\n",
    "            # Reconstructed Distribution\n",
    "            dist_recon:Normal = Normal(pred_result['recon_mu'], pred_result['recon_sigma'])\n",
    "                        \n",
    "            # reshape x to match dist_recon.scale.shape\n",
    "            x = x.view(dist_recon.scale.shape) # [batch_size, 784] | [batch_size, 121]\n",
    "\n",
    "            # .log_prob(x) [batch_size, 784] | [batch_size, 121] -> mean() [batch_size] \n",
    "            log_lik = dist_recon.log_prob(x).mean(dim=1) \n",
    " \n",
    "            recon_loss =  log_lik\n",
    "\n",
    "        # normal/deterministic VAE\n",
    "        else:\n",
    "            recon_x:Tensor = pred_result['decoded']\t\n",
    "            mse_loss_list = []\n",
    "            \n",
    "            for (xi, recon_xi) in zip(x, recon_x):\n",
    "                mse = mse_loss(recon_xi, xi, reduction='sum')\n",
    "                mse_loss_list.append(mse.item())  \n",
    "\n",
    "            mse_loss_batch = torch.Tensor(mse_loss_list) \n",
    "            recon_loss = mse_loss_batch\n",
    "        return recon_loss\n",
    "\n",
    "    def get_loss(self, x:Tensor, pred_result:dict) -> dict:\n",
    "        loss_dict:dict = dict()\n",
    "\n",
    "        # probablistic VAE\n",
    "        if self.is_probabilistic:\n",
    "\n",
    "            # Distributions\n",
    "            dist_latent:Normal = pred_result['latent_dist']\n",
    "            dist_recon:Normal = Normal(pred_result['recon_mu'], pred_result['recon_sigma'])\n",
    "                        \n",
    "            # reshape x to match dist_recon.scale.shape\n",
    "            x = x.view(dist_recon.scale.shape) # [batch_size, 784] | [batch_size, 121]\n",
    "\n",
    "            # .log_prob(x) [batch_size, 784] | [batch_size, 121] -> mean() [784] | [121] -> sum()/mean single value\n",
    "            log_lik = dist_recon.log_prob(x).mean(dim=1).sum() # mean or sum or ?\n",
    "            \n",
    "            kl = kl_divergence(dist_latent, self.prior).mean(dim=0).sum() # single value\n",
    "\n",
    "            loss = kl - log_lik\n",
    "            loss_dict['kl'] = kl\n",
    "            loss_dict['recon_loss'] = log_lik\n",
    "            loss_dict['loss'] = loss\n",
    "\n",
    "        # normal/deterministic VAE\n",
    "        else:\n",
    "            recon_x:Tensor = pred_result['decoded']\t\n",
    "            posterior:Normal = pred_result['latent_dist']\n",
    "\n",
    "            # LOSS\n",
    "            recon_loss = mse_loss(recon_x, x, reduction='sum')\n",
    "            kl = kl_divergence(posterior, self.prior).sum()\n",
    "            loss = kl + recon_loss\n",
    "\n",
    "            loss_dict['kl'] = kl\n",
    "            loss_dict['recon_loss'] = recon_loss\n",
    "            loss_dict['loss'] = loss\n",
    "\n",
    "        return loss_dict\n",
    "    \n",
    "    #=================[RECONSTRUCTION]==============\n",
    "    def reconstruct(self, x: Tensor) -> Tensor:\n",
    "        '''\n",
    "        batch of input data x\n",
    "\n",
    "        returns batch of decoded data (reconstructed)\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            pred = self.predict(x)\n",
    "            reconstructions = pred['decoded']\n",
    "\n",
    "        return reconstructions\n",
    "\n",
    "\n",
    "    #=================[ANOMALY DETECTION]==============\n",
    "    def is_anomaly(self, x: Tensor, alpha: float = 0.05):\n",
    "        '''\n",
    "            case 1: pVAE anomaly if recon_prob  < alpha\n",
    "            case 2: dVAE anomaly if recon_error > alpha\n",
    "        '''\n",
    "        # predict\n",
    "        with torch.no_grad():\n",
    "            pred:dict = self.predict(x)\n",
    "\n",
    "        # Probabilistic VAE (pVAE)\n",
    "        if self.is_probabilistic:\n",
    "            # Compute the reconstruction probability for each instance in the batch\n",
    "            recon_dist = Normal(pred['recon_mu'], pred['recon_sigma'])\n",
    "            x = x.unsqueeze(0)  # Add an extra dimension to match the batch size\n",
    "            x_len_shape = len(x.shape)\n",
    "            \n",
    "            # Calculate the log probabilities and take the exponential\n",
    "            log_probs = recon_dist.log_prob(x).exp()\n",
    "            \n",
    "            # Compute the mean along dimensions other than the batch dimension\n",
    "            for _ in range(x_len_shape - 2):\n",
    "                log_probs = log_probs.mean(dim=-1)\n",
    "\n",
    "            # Sum over the remaining dimension to get the individual instance probabilities\n",
    "            recon_prob = log_probs.sum(dim=-1)\n",
    "\n",
    "            # Determine if each instance is an anomaly based on the threshold (alpha)\n",
    "            is_ano = recon_prob < alpha\n",
    "            return is_ano, recon_prob\n",
    "\n",
    "        # Deterministic VAE (dVAE)\n",
    "        else:\n",
    "            # Calculate the reconstruction error for each instance in the batch using MSE\n",
    "            recon_x = pred['decoded']\n",
    "            \n",
    "            # Compute the MSE error for each instance\n",
    "            mse_error = mse_loss(recon_x, x, reduction='sum')\n",
    "            \n",
    "            # Compute the mean along dimensions other than the batch dimension\n",
    "            for _ in range(len(mse_error.shape) - 1):\n",
    "                mse_error = mse_error.mean(dim=-1)\n",
    "\n",
    "            # Determine if each instance is an anomaly based on the threshold (alpha)\n",
    "            is_ano = mse_error > alpha\n",
    "            return is_ano, mse_error\n",
    "       \n",
    "class VAE_CNN(IVAE):\n",
    "    def __init__(\n",
    "                self,\n",
    "                is_probabilistic:bool,\n",
    "                io_size:int=784,\n",
    "                latent_size:int=15\n",
    "                ) -> None:\n",
    "        super().__init__(\n",
    "            is_probabilistic=is_probabilistic,\n",
    "            io_size=io_size, \n",
    "            latent_size=latent_size\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    def get_architecture(self)  -> VAEArchitecture:\n",
    "        architecture_3layer = VAEArchitecture(\n",
    "            # ENCODER\n",
    "            encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 3x3 kernel\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(32),  \n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 3x3 kernel\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(64),  \n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # 3x3 kernel, stride 2 for downsampling\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(128), \n",
    "                nn.Flatten(),\n",
    "            ),\n",
    "            # LATENT SPACE\n",
    "            latent_mu=nn.Linear(128 * 14 * 14, self.latent_size),\n",
    "            latent_sigma=nn.Linear(128 * 14 * 14, self.latent_size),\n",
    "            # DECODER\n",
    "            decoder = nn.Sequential(\n",
    "                nn.Linear(self.latent_size, 128 * 14 * 14),\n",
    "                nn.Unflatten(1, (128, 14, 14)),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 4x4 kernel\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(64), \n",
    "                nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),  # 3x3 kernel\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(32),  \n",
    "                nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1),  # 3x3 kernel\n",
    "                nn.Sigmoid(),\n",
    "            ),\n",
    "            # RECONSTRUCTION\n",
    "            recon_mu= None if not self.is_probabilistic else nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(28 * 28, self.io_size)\n",
    "            ),\n",
    "            recon_sigma= None if not self.is_probabilistic else nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(28 * 28, self.io_size)\n",
    "            ) \n",
    "        )\n",
    "        \n",
    "        return architecture_3layer\n",
    "    \n",
    "class VAE_Tabular(IVAE):\n",
    "    def __init__(\n",
    "            self,\n",
    "            is_probabilistic:bool,\n",
    "            io_size:int=121,\n",
    "            latent_size:int=10\n",
    "            ) -> None:\n",
    "        super().__init__(\n",
    "            is_probabilistic=is_probabilistic,\n",
    "            io_size=io_size, \n",
    "            latent_size=latent_size,\n",
    "        )\n",
    "        return \n",
    "\n",
    "    def get_architecture(self)  -> VAEArchitecture:\n",
    "        architecture:VAEArchitecture = VAEArchitecture(\n",
    "            # ENCODER\n",
    "            encoder = nn.Sequential(\n",
    "                    nn.Linear(self.io_size // 1, self.io_size // 2, dtype=torch.float32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(self.io_size // 2, self.io_size // 4, dtype=torch.float32),\n",
    "                    nn.ReLU()\n",
    "            ),\n",
    "            # LATENT SPACE\n",
    "            latent_mu     = torch.nn.Linear(self.io_size // 4, self.latent_size, dtype=torch.float32),\n",
    "            latent_sigma  = torch.nn.Linear(self.io_size // 4, self.latent_size, dtype=torch.float32),\n",
    "            \n",
    "            # DECODER\n",
    "            decoder = nn.Sequential(\n",
    "                nn.Linear(self.latent_size, self.io_size // 4, dtype=torch.float32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.io_size // 4, self.io_size // 2, dtype=torch.float32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.io_size // 2, self.io_size, dtype=torch.float32),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "\n",
    "            # RECONSTRUCTION\n",
    "            recon_mu     = None if not self.is_probabilistic else nn.Linear(\n",
    "                self.io_size // 1, self.io_size // 1, dtype=torch.float32\n",
    "            ),\n",
    "            recon_sigma  = None if not self.is_probabilistic else nn.Linear(\n",
    "                self.io_size // 1, self.io_size // 1, dtype=torch.float32\n",
    "            )\n",
    "        )\n",
    "        return architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha(\n",
    "        device:str, \n",
    "        model:IVAE,\n",
    "        loader_train:DataLoader,\n",
    "        model_to_train=ModelToTrain \n",
    "    ) -> float :\n",
    "    print(f\"\\tDetecting alpha (threshold).\\n\\tModel is {'PROBABILISTIC' if model.is_probabilistic else 'DETERMINISTIC'}, so loss {'(i.e., recon. prob.)' if model.is_probabilistic else '(i.e., recon. error)'} of ANOMALIES should to be {'SMALLER' if model.is_probabilistic else 'BIGGER'} than alpha.\")\n",
    "    alpha:float = 1e10 if model.is_probabilistic else 0\n",
    "\n",
    "    labels:List[int] = loader_train.dataset.tensors[1].unique().tolist()\n",
    "    loss_per_class:dict = {label:[] for label in labels}    \n",
    "\n",
    "    # 1. calc loss of each training instance\n",
    "    for x_batch, y_batch in loader_train:\n",
    "        x_batch = x_batch.to(device)\n",
    "\n",
    "        # calc loss\n",
    "        with torch.no_grad():\n",
    "            pred_dict:dict = model.predict(x_batch)\n",
    "            rec_loss:Tensor = model.get_reconstruction_loss(x_batch, pred_dict) # [batch_size]\n",
    "\n",
    "        # append loss to list\n",
    "        for i, label in enumerate(y_batch):\n",
    "            loss_per_class[label.item()].append(rec_loss[i].item())\n",
    "\n",
    "    # Dataframe to see loss distribution per class\n",
    "    df_list = []  # Liste der DataFrames für jede Klasse\n",
    "    for label in labels:\n",
    "        df_batch = pd.DataFrame({\n",
    "            'label': [label],\n",
    "            'avg_loss': [sum(loss_per_class[label]) / len(loss_per_class[label])],\n",
    "            'min_loss': [min(loss_per_class[label])],\n",
    "            'max_loss': [max(loss_per_class[label])]\n",
    "        })\n",
    "        df_list.append(df_batch)\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # if KDD 1999, map label to str using LabelsKDD1999\n",
    "    if model_to_train == ModelToTrain.FULLY_TABULAR:\n",
    "        df = _df_label_mapping_kdd1999(df)\n",
    "\n",
    "    print('\\n\\tLoss distribution per class (TRAIN):')\n",
    "    df = df.sort_values(by=['avg_loss'], ascending=model.is_probabilistic)\n",
    "    print(df.head(20))\n",
    "\n",
    "    # Choose alpha: lowestes recon. prob / highestes recon. error\n",
    "    alpha:float = df['avg_loss'].min() if model.is_probabilistic else df['avg_loss'].max()\n",
    "    print(f'\\n\\t--> Alpha: {alpha} {\"(lowest avg. recon. prob.)\" if model.is_probabilistic else \"(higest avg. recon error)\"}\\n')\n",
    "\n",
    "    return alpha\n",
    "\n",
    "def detect_anomalies(\n",
    "        device:str, \n",
    "        model:IVAE,\n",
    "        loader_train:DataLoader, \n",
    "        loader_test:DataLoader,\n",
    "        model_to_train:ModelToTrain\n",
    ") -> None:\n",
    "    '''\n",
    "    1. determine alpha based on TRAINING data\n",
    "    2. detect anomalies in TEST data based on alpha\n",
    "    3. show anomalies distribution (i.e., how many anomalies are detected in each class)\n",
    "    '''\n",
    "    model.eval()  \n",
    "\n",
    "    # alpha\n",
    "    alpha:float = get_alpha(device, model, loader_train, model_to_train)\n",
    "\n",
    "    # anomalies\n",
    "    df_anomalies:pd.DataFrame = pd.DataFrame(\n",
    "        columns=['label', 'loss', 'is_anomaly']\n",
    "    )\n",
    "\n",
    "    for x_batch, y_batch in loader_test:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # calc loss\n",
    "        with torch.no_grad():\n",
    "            pred_dict:dict = model.predict(x_batch)\n",
    "            rec_loss:dict = model.get_reconstruction_loss(x_batch, pred_dict)\n",
    "        # anomalies\n",
    "        is_anomaly_bitmask:torch.Tensor = rec_loss < alpha if model.is_probabilistic else rec_loss > alpha\n",
    "        \n",
    "        # create batch dataframe  \n",
    "        df_batch = pd.DataFrame({\n",
    "            'label': y_batch.cpu().numpy(),\n",
    "            'loss': rec_loss.cpu().numpy(),\n",
    "            'is_anomaly': is_anomaly_bitmask.cpu().numpy()\n",
    "        })\n",
    "\n",
    "        # append to anomalies dataframe\n",
    "        df_anomalies = pd.concat([df_anomalies, df_batch])\n",
    "\n",
    "    if model_to_train == ModelToTrain.FULLY_TABULAR:\n",
    "        df_anomalies = _df_label_mapping_kdd1999(df_anomalies)\n",
    "\n",
    "    # avg loss per class\n",
    "    df_new = df_anomalies.groupby('label')['loss'].mean().reset_index()\n",
    "    df_new.columns = ['label', 'avg_loss']\n",
    "\n",
    "    print('\\t\\nLoss distribution per class (TEST):')\n",
    "    df_new = df_new.sort_values(by=['avg_loss'], ascending=model.is_probabilistic)\n",
    "    print(df_new.head(20))\n",
    "\n",
    "\n",
    "    # anomalies distribution \n",
    "    print('\\n\\tAnomalies distribution (TEST):')\n",
    "    df_result = df_anomalies.groupby(['label', 'is_anomaly']).size().reset_index(name='amount')\n",
    "    total_count = df_result.groupby('label')['amount'].transform('sum')\n",
    "    df_result['percentage'] = (df_result['amount'] / total_count * 100).round(2)\n",
    "    \n",
    "    # sort df by 'is_anomaly', 'percentage'\n",
    "    df_result = df_result.sort_values(\n",
    "        by=['is_anomaly', 'percentage'], \n",
    "        ascending=[False, False]\n",
    "    )\n",
    "    print(df_result.head(20))\n",
    "    return \n",
    "\n",
    "\n",
    "def _df_label_mapping_kdd1999(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    label_mapping:dict = {si.value.encoded: si.value.label for si in LabelsKDD1999}\n",
    "    df['label'] = df['label'].map(label_mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_train_pred(log_train_pred:VAELogTrain):\n",
    "    log_train_pred.loss = torch.stack(log_train_pred.loss).cpu().detach().numpy()\n",
    "    log_train_pred.kl = torch.stack(log_train_pred.kl).cpu().detach().numpy()\n",
    "    log_train_pred.recon_loss = torch.stack(log_train_pred.recon_loss).cpu().detach().numpy()\n",
    "\n",
    "    _, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    #turn into scatter plot \n",
    "    ax[0].scatter(range(len(log_train_pred.loss)), log_train_pred.loss, s=1)\n",
    "    ax[0].set_title('Loss')\n",
    "    ax[1].scatter(range(len(log_train_pred.kl)), log_train_pred.kl, s=1)\n",
    "    ax[1].set_title('KL')\n",
    "    ax[2].scatter(range(len(log_train_pred.recon_loss)), log_train_pred.recon_loss, s=1)\n",
    "    ax[2].set_title('Reconstruction Loss')\n",
    "\n",
    "    print(f'\\t\\tPlotting training progress\\n')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return\n",
    "\n",
    "def plot_mnist_orig_and_recon(\n",
    "    batch_size:int,\n",
    "    x_orig:torch.Tensor,\n",
    "    x_recon:torch.Tensor,\n",
    "    y:torch.Tensor,\n",
    ")-> None: \n",
    "    # show original and reconstructed images next to each other\n",
    "    _, axes = plt.subplots(batch_size, 2, figsize=(10, 20))\n",
    "    for i in range(batch_size):\n",
    "        axes[i,0].imshow(x_orig[i].squeeze().detach().cpu().numpy(), cmap='gray')\n",
    "        axes[i,1].imshow(x_recon[i].squeeze().detach().cpu().numpy(), cmap='gray')\n",
    "        axes[i,0].set_title(f'Original {y[i]}')\n",
    "        axes[i,1].set_title(f'Reconstructed {y[i]}')\n",
    "        axes[i,0].axis('off')\n",
    "        axes[i,1].axis('off')\n",
    "\n",
    "    print(f'\\t\\tPlotting MNIST (original and reconstruction)')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return \n",
    "\n",
    "def plot_roc_curve(\n",
    "    model_to_train:ModelToTrain,\n",
    "    anomaly_class_label:str,\n",
    "    fpr:np.ndarray,\n",
    "    tpr:np.ndarray,\n",
    "    auc_score:float,\n",
    ")-> None:\n",
    "    dataset_name = \"mnist\" if model_to_train == ModelToTrain.CNN_MNIST else \"kdd\"\n",
    "    directory = f'roc_{dataset_name}'\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    print(f'\\t\\tPlotting ROC curve')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_loss(\n",
    "        model:IVAE,\n",
    "        loader_test:DataLoader,\n",
    "        device:str,\n",
    "\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    model.eval()\n",
    "    df:pd.DataFrame = pd.DataFrame(columns=['label', 'loss'])\n",
    "\n",
    "    for X, y in loader_test:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # calc loss\n",
    "        with torch.no_grad():\n",
    "            pred_dict:dict = model.predict(X)\n",
    "            rec_loss:dict = model.get_reconstruction_loss(X, pred_dict)\n",
    "        \n",
    "        # create batch dataframe  \n",
    "        df_batch = pd.DataFrame({\n",
    "            'label': y.cpu().numpy(),\n",
    "            'loss':  rec_loss.cpu().numpy(),\n",
    "        })\n",
    "        df = pd.concat([df, df_batch])\n",
    "\n",
    "    # normalize loss\n",
    "    df['loss_normalized'] = df['loss'].round(3)\n",
    "\n",
    "    min_loss = df['loss'].min()\n",
    "    max_loss = df['loss'].max()\n",
    "    df['loss_normalized'] = ((df['loss'] - min_loss) / (max_loss - min_loss)).round(3)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_metrics(\n",
    "    model:IVAE,\n",
    "    loader_test:DataLoader,\n",
    "    device:str,\n",
    "    anomaly_class:Enum,\n",
    "    model_to_train:ModelToTrain,\n",
    "    plot_roc:bool = True,    \n",
    "):\n",
    "    print('EVALLUATION:')\n",
    "\n",
    "    # test data loss\n",
    "    df:pd.DataFrame = get_test_data_loss(\n",
    "        model=model,\n",
    "        loader_test=loader_test,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # add is_anomaly_class column to df\n",
    "    anomaly_class_value = anomaly_class.value if model_to_train == ModelToTrain.CNN_MNIST else anomaly_class.value.encoded\n",
    "    anomaly_class_label = anomaly_class.value if model_to_train == ModelToTrain.CNN_MNIST else anomaly_class.value.label\n",
    "    df['is_anomaly_class'] = df['label'] == anomaly_class_value\n",
    "    \n",
    "    #1 AUC ROC\n",
    "    # calc roc curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true=df['is_anomaly_class'],y_score=df['loss_normalized'],)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    \n",
    "    # plot ROC CURVE\n",
    "    if plot_roc:\n",
    "        plot_roc_curve(model_to_train,anomaly_class_label,fpr,tpr,auc_score)\n",
    "\n",
    "    # 2. F1 scores\n",
    "    f1_scores = [f1_score(df['is_anomaly_class'], df['loss_normalized'] > threshold) for threshold in thresholds]\n",
    "    f1_max = np.max(f1_scores)\n",
    "\n",
    "    # 3. AUC PRC\n",
    "    auc_prc = average_precision_score(df['is_anomaly_class'], df['loss_normalized'])\n",
    "\n",
    "    # 4. add column 'showing how much percent of test data is anomaly class'\n",
    "    _,y =  loader_test.dataset.tensors\n",
    "    len_test_data = len(y)\n",
    "    len_ano_class = len(y[y == anomaly_class_value])\n",
    "    percent_ano_class = (len_ano_class / len_test_data)\n",
    "\n",
    "    return auc_score, f1_max, auc_prc, percent_ano_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(\n",
    "        model:IVAE, \n",
    "        model_to_train:ModelToTrain,\n",
    "        device:str, \n",
    "        num_epochs:int, \n",
    "        optimizer:Optimizer, \n",
    "        lr_scheduler:StepLR,\n",
    "        train_loader:DataLoader, \n",
    "    ):\n",
    "    print('TRAINING:')\n",
    "\n",
    "    # LOGGING\n",
    "    log_train_preds:VAELogTrain = VAELogTrain(\n",
    "        loss=[], \n",
    "        kl=[] ,\n",
    "        recon_loss=[] \n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (X, y) in enumerate(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\t\n",
    "\n",
    "            # FORWARD PASS\n",
    "            forward_dict:dict = model(X)\n",
    "           \n",
    "            # LOSS\n",
    "            loss = forward_dict['loss']\n",
    "\n",
    "            # BACKWARD PASS\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # UPDATE MODEL PARAMETERS\n",
    "            optimizer.step()\n",
    "\n",
    "            # LOGGING\n",
    "            log_train_preds.loss.append(loss)\n",
    "            log_train_preds.kl.append(forward_dict['kl'])\n",
    "            log_train_preds.recon_loss.append(forward_dict['recon_loss'])\n",
    "\n",
    "            if batch_idx % 500 == 0:\n",
    "                print('\\t\\tEpoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f | KL: %.4f | RecLoss: %.4f'\n",
    "                    % (epoch+1, num_epochs, batch_idx,\n",
    "                        len(train_loader), loss, forward_dict['kl'], forward_dict['recon_loss']))\n",
    "\n",
    "\n",
    "        # RECONSTRUCTION PLOT\n",
    "        if model_to_train == ModelToTrain.CNN_MNIST:\n",
    "            print('\\t\\tPlot reconstruction after epoch: %d' % (epoch + 1))\n",
    "            batch_reconstructions:torch.Tensor = model.reconstruct(x=X)\n",
    "            batch_reconstructions  = batch_reconstructions.squeeze(1)\n",
    "            plot_mnist_orig_and_recon(\n",
    "                    batch_size=len(X), \n",
    "                    x_orig=X, \n",
    "                    x_recon=batch_reconstructions,\n",
    "                    y=y, \n",
    "                ) \n",
    "\n",
    "        print('\\t\\tTime elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "        print('\\n') \n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print('\\t\\tTotal Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    # PLOT TRAINING PROGRESS\n",
    "    plot_train_pred(log_train_preds)\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_one_anomaly_class(\n",
    "    # MODEL & ANOMALY CLASS\n",
    "    is_model_probabilistic:bool = False,\n",
    "    model_to_train:ModelToTrain = ModelToTrain.FULLY_TABULAR,\n",
    "    anomaly_class:Enum = LabelsKDD1999.Normal,\n",
    "    # DEVICE\n",
    "    device:str = 'cuda:1',\n",
    "    num_workers:int = 1,\n",
    "    # HYPERPARAMETER\n",
    "    num_epochs:int = 1,\n",
    "    batch_size:int = 128,\n",
    "    learning_rate:float = 1e-5,\n",
    "\n",
    "\n",
    "):\n",
    "    \n",
    "\n",
    "\n",
    "    # LOAD DATA (full; no split)\n",
    "    data:IDataset = DatasetMNIST(is_debug=True)  if model_to_train == ModelToTrain.CNN_MNIST else DatasetKDD(is_debug=True)\n",
    "    dataset_train:TensorDataset = None \n",
    "    dataset_test:TensorDataset = None \n",
    "    dataset_train, dataset_test = data.get_data(anomaly_class=anomaly_class)\n",
    "    \n",
    "    loader_train:DataLoader = DataLoader(\n",
    "        dataset_train, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    loader_test:DataLoader = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    X, y = dataset_train.tensors \n",
    "    len = X.shape[0]\n",
    "\n",
    "    X_test, y_test = dataset_test.tensors\n",
    "    len_test = X_test.shape[0]\n",
    "\n",
    "    print(f'''HYPERPARAMETER:\n",
    "    \\tProbabilistic:\\t{is_model_probabilistic}\n",
    "    \\tModel:\\t\\t\\t{model_to_train}\n",
    "    \\tAnomaly class:\\t{anomaly_class.value}\n",
    "    \\tEpochs:\\t\\t\\t{num_epochs}\n",
    "    \\tBatch size:\\t\\t{batch_size}\n",
    "    \\tLearning rate:\\t{learning_rate}\n",
    "    \\tLength of training dataset {len}\n",
    "    \\tLength of test dataset {len_test}\n",
    "    ''')\n",
    "\n",
    "\n",
    "\n",
    "    # MODEL\n",
    "    model:nn.Module = None \n",
    "    if model_to_train == ModelToTrain.CNN_MNIST:\n",
    "        # LOGGING: show data properties (shapes, img resolution)\n",
    "        img_resolution = (X.shape[2], X.shape[3])\n",
    "        model:VAE_CNN = VAE_CNN(\n",
    "            is_probabilistic=is_model_probabilistic,\n",
    "            io_size=(img_resolution[0] * img_resolution[1])\n",
    "        )\n",
    "\n",
    "        print(f'''DATA SHAPE:\n",
    "        Length of dataset {len}\n",
    "        Labels shape: {y.shape}\n",
    "        Images shape: {X.shape}\n",
    "        Img resolution is {img_resolution}={img_resolution[0]*img_resolution[1]}\n",
    "        ''')\n",
    "\n",
    "    elif model_to_train == ModelToTrain.FULLY_TABULAR:\n",
    "        model:VAE_Tabular = VAE_Tabular(is_probabilistic=is_model_probabilistic) \n",
    "\n",
    "    else:\n",
    "        raise Exception('Invalid model to train')\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # OPTIMIZER\n",
    "    OPTIMIZER:Adam = Adam(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate,\n",
    "    )\n",
    "    LR_SCHEDULER = StepLR(OPTIMIZER, step_size=5, gamma=0.1)  \n",
    "\n",
    "    # TRAINING\n",
    "    train(\n",
    "        model=model, \n",
    "        model_to_train=model_to_train,\n",
    "        device=device, \n",
    "        num_epochs=num_epochs ,\n",
    "        optimizer=OPTIMIZER, \n",
    "        lr_scheduler=LR_SCHEDULER,\n",
    "        train_loader=loader_train,\n",
    "    )\n",
    "    \n",
    "    # EVALUATION\n",
    "    auc, f1, auc_prc, ano_class_percentage = get_metrics(\n",
    "        model=model,\n",
    "        loader_test=loader_test,\n",
    "        device=device,\n",
    "        anomaly_class=anomaly_class,\n",
    "        model_to_train=model_to_train,\n",
    "        plot_roc=True,\n",
    "    )\n",
    "    \n",
    "    print(f'\\t\\tAUC value = {auc.round(3)}')\n",
    "    print(f'\\t\\tF1 value = {f1.round(3)}')\n",
    "    print(f'\\t\\tAUC PRC value = {auc_prc.round(3)}')\n",
    "\n",
    "    # ANOMALY DETECTION\n",
    "    #print('ANOMALY DETECTION')\n",
    "#\n",
    "    #detect_anomalies(\n",
    "    #    model=model,\n",
    "    #    device=device,\n",
    "    #    loader_train=loader_train,\n",
    "    #    loader_test=loader_test,\n",
    "    #    model_to_train=model_to_train\n",
    "    #)\n",
    "\n",
    "    return auc, f1, auc_prc, ano_class_percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS ID:\t\t109470\n",
      "\n",
      "DEVICE:\t\tcuda:1 with 4 workers.\n",
      "\n",
      "RUNNING: FOR ANOMALY CLASS:  Normal\n",
      "------------------------------------------------\n",
      "LOADING DATA:\n",
      "\t\t(✓) downloaded labeled data\n",
      "\t\t(✓) downloaded attack type data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t(✓) fixed dtypes (int -> str) \n",
      "        \twith len =\t\t9 \n",
      "        \twith columns =\t['Attack Type' 'label' 'protocol_type' 'service' 'flag' 'land' 'logged_in'\n",
      " 'is_host_login' 'is_guest_login']\n",
      "\t\t(✓) normalized numerical columns into [0, 1] range\n",
      "\t\t(✓) one-hot encoded categorical columns\n",
      "\t\t(✓) Training set only contains NORMALS, NO ANOMALIES CLASS normal.\n",
      "\t\t\tlabels:\t['dos', 'probe', 'r2l', 'u2r']\n",
      "\t\t(✓) Test set contains NORAMLS and ANOMALY CLASS normal.\n",
      "\t\t\tlabels:\t['normal', 'dos', 'r2l', 'probe', 'u2r']\n",
      "\t\t(✓) casted DataFrame into X, y (y is one-hot encoded)\n",
      "\t\t(✓) casted DataFrame into X, y (y is one-hot encoded)\n",
      "\t\t(✓) casted X,y to TensorDataset\n",
      "\t\t(✓) casted X,y to TensorDataset\n",
      "\t\t(✓) loaded data\n",
      "\n",
      "HYPERPARAMETER:\n",
      "    \tProbabilistic:\tTrue\n",
      "    \tModel:\t\t\tModelToTrain.FULLY_TABULAR\n",
      "    \tAnomaly class:\tStrIntMapping(label='normal', encoded=0)\n",
      "    \tEpochs:\t\t\t1\n",
      "    \tBatch size:\t\t128\n",
      "    \tLearning rate:\t1e-05\n",
      "    \tLength of training dataset 315800\n",
      "    \tLength of test dataset 176018\n",
      "    \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Adam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb Cell 22\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m: \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     main()\n",
      "\u001b[1;32m/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m f1:\u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m ano_class_percentage: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m auc, f1, auc_prc, ano_class_percentage \u001b[39m=\u001b[39m run_for_one_anomaly_class(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39m# MODEL & ANOMALY CLASS\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     is_model_probabilistic\u001b[39m=\u001b[39;49mIS_MODEL_PROBABILISTIC,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     model_to_train\u001b[39m=\u001b[39;49mMODEL_TO_TRAIN,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     anomaly_class\u001b[39m=\u001b[39;49manomaly_class,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m# DEVICE\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     device\u001b[39m=\u001b[39;49mDEVICE,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     num_workers\u001b[39m=\u001b[39;49mNUM_WORKERS,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m# HYPERPARAMETER\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mLEARNING_RATE,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m df_temp:pd\u001b[39m.\u001b[39mDataFrame \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39manomaly_class\u001b[39m\u001b[39m'\u001b[39m: [anomaly_class\u001b[39m.\u001b[39mname],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mauc\u001b[39m\u001b[39m'\u001b[39m: [auc\u001b[39m.\u001b[39mround(\u001b[39m3\u001b[39m)],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39manomaly_class_\u001b[39m\u001b[39m%\u001b[39m\u001b[39m_in_test_data\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39mround\u001b[39m(ano_class_percentage, \u001b[39m3\u001b[39m)]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m })\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df, df_temp], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb Cell 22\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# OPTIMIZER\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m OPTIMIZER:Adam \u001b[39m=\u001b[39m Adam(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     model\u001b[39m.\u001b[39mparameters(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m     lr\u001b[39m=\u001b[39mlearning_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m LR_SCHEDULER \u001b[39m=\u001b[39m StepLR(OPTIMIZER, step_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)  \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmorty.imp.fu-berlin.de/srv/data/joeh97/github/uni23ss_VAE-anomaly-detection/code/scripts/submission-vae.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39m# TRAINING\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Adam' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    print(f'PROCESS ID:\\t\\t{os.getpid()}\\n')\n",
    "\n",
    "    IS_MODEL_PROBABILISTIC = True\n",
    "    MODEL_TO_TRAIN = ModelToTrain.FULLY_TABULAR\n",
    "\n",
    "    # DEVICE\n",
    "    CUDA_DEVICE_NUM = 1\n",
    "    DEVICE = torch.device(f'cuda:{CUDA_DEVICE_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "    NUM_WORKERS = 1 if DEVICE == 'cpu' else 4\n",
    "    print(f'DEVICE:\\t\\t{DEVICE} with {NUM_WORKERS} workers.\\n')\n",
    "\n",
    "    # HYPERPARAMETER\n",
    "    if MODEL_TO_TRAIN == ModelToTrain.CNN_MNIST:\n",
    "        NUM_EPOCHS = 2  if DEVICE == 'cpu' else 1\n",
    "        BATCH_SIZE = 16 if DEVICE == 'cpu' else 64\n",
    "        LEARNING_RATE = 1e-4\n",
    "    \n",
    "    elif MODEL_TO_TRAIN == ModelToTrain.FULLY_TABULAR:\n",
    "        NUM_EPOCHS = 2  if DEVICE == 'cpu' else 1\n",
    "        BATCH_SIZE = 16 if DEVICE == 'cpu' else 128 \n",
    "        LEARNING_RATE = 1e-5\n",
    "\n",
    "    else:\n",
    "        raise Exception('Invalid model to train')\n",
    "    \n",
    "    # ANOMALY CLASS\n",
    "    anomaly_enum:Enum = LabelsKDD1999 if MODEL_TO_TRAIN == ModelToTrain.FULLY_TABULAR else LabelsMNIST\n",
    "    anomaly_classes:List[Enum] = [ano_class for ano_class in anomaly_enum]  \n",
    "\n",
    "    # run for each anomaly class and track metrics\n",
    "    df = pd.DataFrame(columns=['anomaly_class', 'auc', 'auc_prc', 'f1', 'anomaly_class_%_in_test_data'])\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for anomaly_class in anomaly_classes:\n",
    "\n",
    "        print(f'RUNNING: FOR ANOMALY CLASS: ', anomaly_class.name)\n",
    "        print('------------------------------------------------')\n",
    "        auc:float = None\n",
    "        auc_prc:float = None\n",
    "        f1:float = None\n",
    "        ano_class_percentage: float = None\n",
    "\n",
    "\n",
    "        auc, f1, auc_prc, ano_class_percentage = run_for_one_anomaly_class(\n",
    "            # MODEL & ANOMALY CLASS\n",
    "            is_model_probabilistic=IS_MODEL_PROBABILISTIC,\n",
    "            model_to_train=MODEL_TO_TRAIN,\n",
    "            anomaly_class=anomaly_class,\n",
    "            # DEVICE\n",
    "            device=DEVICE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            # HYPERPARAMETER\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "        df_temp:pd.DataFrame = pd.DataFrame({\n",
    "            'anomaly_class': [anomaly_class.name],\n",
    "            'auc': [auc.round(3)],\n",
    "            'auc_prc': [auc_prc.round(3)],\n",
    "            'f1': [f1.round(3)],\n",
    "            'anomaly_class_%_in_test_data': [round(ano_class_percentage, 3)]\n",
    "        })\n",
    "\n",
    "        df = pd.concat([df, df_temp], ignore_index=True)\n",
    "\n",
    "        print('\\n\\nTemporary overview about metrics:\\n')\n",
    "        print(df.head(100))\n",
    "\n",
    "        print('\\n\\nTOTAL TIME ELAPSED: %.2f min' % ((time.time() - start_time)/60))\n",
    "        print('================================================\\n\\n\\n\\n\\n')\n",
    "\n",
    "    print('FINAL RESULT')\n",
    "    print(df.head(100))\n",
    "\n",
    "    print('\\n\\nScript finished.')\n",
    "    return \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni-morty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
